{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walmart Store Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: The objective is predicting store sales using historical markdown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data by PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"myApp\").config(\"spark.mongodb.input.uri\",\"mongodb://localhost:27017/project.walmart?readPreference=primaryPreferred\").config(\"spark.mongodb.output.uri\",\"mongodb://localhost:27017/project.walmart\").config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/features.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"features.csv\")\n",
    "features=spark.read.csv(\"file:///\"+SparkFiles.get(\"features.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/stores.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"stores.csv\")\n",
    "stores=spark.read.csv(\"file:///\"+SparkFiles.get(\"stores.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/train.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"train.csv\")\n",
    "train=spark.read.csv(\"file:///\"+SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/test.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"test.csv\")\n",
    "test=spark.read.csv(\"file:///\"+SparkFiles.get(\"test.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------+---------+\n",
      "|Store|Dept|      Date|Weekly_Sales|IsHoliday|\n",
      "+-----+----+----------+------------+---------+\n",
      "|    1|   1|2010-02-05|     24924.5|    false|\n",
      "|    1|   1|2010-02-12|    46039.49|     true|\n",
      "+-----+----+----------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+---------+\n",
      "|Store|Dept|      Date|IsHoliday|\n",
      "+-----+----+----------+---------+\n",
      "|    1|   1|2012-11-02|    false|\n",
      "|    1|   1|2012-11-09|    false|\n",
      "+-----+----+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|Store|      Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|IsHoliday|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|    1|2010-02-05|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|     true|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|Store|Type|  Size|\n",
      "+-----+----+------+\n",
      "|    1|   A|151315|\n",
      "|    2|   A|202307|\n",
      "+-----+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Date': 0, 'Temperature': 0, 'Fuel_Price': 0, 'MarkDown1': 0, 'MarkDown2': 0, 'MarkDown3': 0, 'MarkDown4': 0, 'MarkDown5': 0, 'CPI': 0, 'Unemployment': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:features.filter(features[col].isNull()).count() for col in features.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Type': 0, 'Size': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:stores.filter(stores[col].isNull()).count() for col in stores.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Dept': 0, 'Date': 0, 'Weekly_Sales': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:train.filter(train[col].isNull()).count() for col in train.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Dept': 0, 'Date': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:test.filter(test[col].isNull()).count() for col in test.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merging the DataSet \n",
    "      -(train+Store+Feature) \n",
    "      -(test+Store+Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bt=train.join(stores, [\"Store\"])\n",
    "train = train_bt.join(features, on=['Store', 'Date','IsHoliday'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bt=test.join(stores,[\"Store\"])\n",
    "test = test_bt.join(features, on=['Store', 'Date','IsHoliday'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|Store|      Date|IsHoliday|Dept|Weekly_Sales|Type|  Size|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|\n",
      "+-----+----------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|    1|2010-02-05|    false|   1|     24924.5|   A|151315|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|\n",
      "|    1|2010-02-12|     true|   1|    46039.49|   A|151315|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|\n",
      "+-----+----------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|Store|      Date|IsHoliday|Dept|Type|  Size|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|\n",
      "+-----+----------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|    1|2012-11-02|    false|   1|   A|151315|      55.32|     3.386|  6766.44|   5147.7|    50.82|   3639.9|  2737.42|223.4627793|       6.573|\n",
      "|    1|2012-11-09|    false|   1|   A|151315|      61.24|     3.314| 11421.32|  3370.89|    40.28|  4646.79|  6154.16|223.4813073|       6.573|\n",
      "+-----+----------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: string (nullable = true)\n",
      " |-- MarkDown2: string (nullable = true)\n",
      " |-- MarkDown3: string (nullable = true)\n",
      " |-- MarkDown4: string (nullable = true)\n",
      " |-- MarkDown5: string (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n",
      "None\n",
      "*****************************************\n",
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: string (nullable = true)\n",
      " |-- MarkDown2: string (nullable = true)\n",
      " |-- MarkDown3: string (nullable = true)\n",
      " |-- MarkDown4: string (nullable = true)\n",
      " |-- MarkDown5: string (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.printSchema())\n",
    "print(\"*****************************************\")\n",
    "print(test.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Data in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = spark.read.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_train\").load()\n",
    "train.write.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_train\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_test = spark.read.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_test\").load()\n",
    "test.write.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_test\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from MongoDB to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',27017)\n",
    "db = client.walmart\n",
    "train = db.data_train\n",
    "train = pd.DataFrame(list(train.find())).drop(['_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',27017)\n",
    "db_test = client.walmart\n",
    "data_test = db.data_test\n",
    "test = pd.DataFrame(list(data_test.find())).drop(['_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 421570 entries, 0 to 421569\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         421570 non-null  int64  \n",
      " 1   Date          421570 non-null  object \n",
      " 2   IsHoliday     421570 non-null  bool   \n",
      " 3   Dept          421570 non-null  int64  \n",
      " 4   Weekly_Sales  421570 non-null  float64\n",
      " 5   Type          421570 non-null  object \n",
      " 6   Size          421570 non-null  int64  \n",
      " 7   Temperature   421570 non-null  float64\n",
      " 8   Fuel_Price    421570 non-null  float64\n",
      " 9   MarkDown1     421570 non-null  object \n",
      " 10  MarkDown2     421570 non-null  object \n",
      " 11  MarkDown3     421570 non-null  object \n",
      " 12  MarkDown4     421570 non-null  object \n",
      " 13  MarkDown5     421570 non-null  object \n",
      " 14  CPI           421570 non-null  object \n",
      " 15  Unemployment  421570 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(3), object(9)\n",
      "memory usage: 48.6+ MB\n",
      "None\n",
      "*****************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115064 entries, 0 to 115063\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         115064 non-null  int64  \n",
      " 1   Date          115064 non-null  object \n",
      " 2   IsHoliday     115064 non-null  bool   \n",
      " 3   Dept          115064 non-null  int64  \n",
      " 4   Type          115064 non-null  object \n",
      " 5   Size          115064 non-null  int64  \n",
      " 6   Temperature   115064 non-null  float64\n",
      " 7   Fuel_Price    115064 non-null  float64\n",
      " 8   MarkDown1     115064 non-null  object \n",
      " 9   MarkDown2     115064 non-null  object \n",
      " 10  MarkDown3     115064 non-null  object \n",
      " 11  MarkDown4     115064 non-null  object \n",
      " 12  MarkDown5     115064 non-null  object \n",
      " 13  CPI           115064 non-null  object \n",
      " 14  Unemployment  115064 non-null  object \n",
      "dtypes: bool(1), float64(2), int64(3), object(9)\n",
      "memory usage: 12.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.info())\n",
    "print (\"*****************************************\")\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Missing Value Treatment like Markdown\n",
    "Imputing it with Zero(No Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['MarkDown1'] = train['MarkDown1'].str.replace('NA','0').astype(np.float64)\n",
    "train['MarkDown2'] = train['MarkDown2'].str.replace('NA','0').astype(np.float64)\n",
    "train['MarkDown3'] = train['MarkDown3'].str.replace('NA','0').astype(np.float64)\n",
    "train['MarkDown4'] = train['MarkDown4'].str.replace('NA','0').astype(np.float64)\n",
    "train['MarkDown5'] = train['MarkDown5'].str.replace('NA','0').astype(np.float64)\n",
    "train['CPI'] = train['CPI'].astype(np.float64)\n",
    "train['Unemployment'] = train['Unemployment'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  IsHoliday  Dept  Weekly_Sales Type    Size  Temperature  \\\n",
       "0      1  2010-02-05      False     1      24924.50    A  151315        42.31   \n",
       "1      1  2010-02-12       True     1      46039.49    A  151315        38.51   \n",
       "2      1  2010-02-19      False     1      41595.55    A  151315        39.93   \n",
       "3      1  2010-02-26      False     1      19403.54    A  151315        46.63   \n",
       "4      1  2010-03-05      False     1      21827.90    A  151315        46.50   \n",
       "\n",
       "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
       "0       2.572        0.0        0.0        0.0        0.0        0.0   \n",
       "1       2.548        0.0        0.0        0.0        0.0        0.0   \n",
       "2       2.514        0.0        0.0        0.0        0.0        0.0   \n",
       "3       2.561        0.0        0.0        0.0        0.0        0.0   \n",
       "4       2.625        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "          CPI  Unemployment  \n",
       "0  211.096358         8.106  \n",
       "1  211.242170         8.106  \n",
       "2  211.289143         8.106  \n",
       "3  211.319643         8.106  \n",
       "4  211.350143         8.106  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store             int64\n",
       "Date             object\n",
       "IsHoliday          bool\n",
       "Dept              int64\n",
       "Weekly_Sales    float64\n",
       "Type             object\n",
       "Size              int64\n",
       "Temperature     float64\n",
       "Fuel_Price      float64\n",
       "MarkDown1       float64\n",
       "MarkDown2       float64\n",
       "MarkDown3       float64\n",
       "MarkDown4       float64\n",
       "MarkDown5       float64\n",
       "CPI             float64\n",
       "Unemployment    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['MarkDown1'] = test['MarkDown1'].str.replace('NA','0').astype(np.float64)\n",
    "test['MarkDown2'] = test['MarkDown2'].str.replace('NA','0').astype(np.float64)\n",
    "test['MarkDown3'] = test['MarkDown3'].str.replace('NA','0').astype(np.float64)\n",
    "test['MarkDown4'] = test['MarkDown4'].str.replace('NA','0').astype(np.float64)\n",
    "test['MarkDown5'] = test['MarkDown5'].str.replace('NA','0').astype(np.float64)\n",
    "test['CPI'] = test['CPI'].str.replace('NA','0').astype(np.float64)\n",
    "test['Unemployment'] = test['Unemployment'].str.replace('NA','0').astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>55.32</td>\n",
       "      <td>3.386</td>\n",
       "      <td>6766.44</td>\n",
       "      <td>5147.70</td>\n",
       "      <td>50.82</td>\n",
       "      <td>3639.90</td>\n",
       "      <td>2737.42</td>\n",
       "      <td>223.462779</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>61.24</td>\n",
       "      <td>3.314</td>\n",
       "      <td>11421.32</td>\n",
       "      <td>3370.89</td>\n",
       "      <td>40.28</td>\n",
       "      <td>4646.79</td>\n",
       "      <td>6154.16</td>\n",
       "      <td>223.481307</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>52.92</td>\n",
       "      <td>3.252</td>\n",
       "      <td>9696.28</td>\n",
       "      <td>292.10</td>\n",
       "      <td>103.78</td>\n",
       "      <td>1133.15</td>\n",
       "      <td>6612.69</td>\n",
       "      <td>223.512911</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>56.23</td>\n",
       "      <td>3.211</td>\n",
       "      <td>883.59</td>\n",
       "      <td>4.17</td>\n",
       "      <td>74910.32</td>\n",
       "      <td>209.91</td>\n",
       "      <td>303.32</td>\n",
       "      <td>223.561947</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>52.34</td>\n",
       "      <td>3.207</td>\n",
       "      <td>2460.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3838.35</td>\n",
       "      <td>150.57</td>\n",
       "      <td>6966.34</td>\n",
       "      <td>223.610984</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  IsHoliday  Dept Type    Size  Temperature  Fuel_Price  \\\n",
       "0      1  2012-11-02      False     1    A  151315        55.32       3.386   \n",
       "1      1  2012-11-09      False     1    A  151315        61.24       3.314   \n",
       "2      1  2012-11-16      False     1    A  151315        52.92       3.252   \n",
       "3      1  2012-11-23       True     1    A  151315        56.23       3.211   \n",
       "4      1  2012-11-30      False     1    A  151315        52.34       3.207   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
       "0    6766.44    5147.70      50.82    3639.90    2737.42  223.462779   \n",
       "1   11421.32    3370.89      40.28    4646.79    6154.16  223.481307   \n",
       "2    9696.28     292.10     103.78    1133.15    6612.69  223.512911   \n",
       "3     883.59       4.17   74910.32     209.91     303.32  223.561947   \n",
       "4    2460.03       0.00    3838.35     150.57    6966.34  223.610984   \n",
       "\n",
       "   Unemployment  \n",
       "0         6.573  \n",
       "1         6.573  \n",
       "2         6.573  \n",
       "3         6.573  \n",
       "4         6.573  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store             int64\n",
       "Date             object\n",
       "IsHoliday          bool\n",
       "Dept              int64\n",
       "Type             object\n",
       "Size              int64\n",
       "Temperature     float64\n",
       "Fuel_Price      float64\n",
       "MarkDown1       float64\n",
       "MarkDown2       float64\n",
       "MarkDown3       float64\n",
       "MarkDown4       float64\n",
       "MarkDown5       float64\n",
       "CPI             float64\n",
       "Unemployment    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'Dept', 'Weekly_Sales', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
      "['Date', 'Type']\n"
     ]
    }
   ],
   "source": [
    "numeric_var_train=[key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "cat_var_train=[key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['object']]\n",
    "# Train Numerical Data\n",
    "train_num=train[numeric_var_train]\n",
    "\n",
    "# Train Categorical Data\n",
    "train_cat=train[cat_var_train]\n",
    "print(numeric_var_train)\n",
    "print(cat_var_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyxll-jupyter in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from pyxll-jupyter) (1.0.0)\n",
      "Requirement already satisfied: notebook>=6.0.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from pyxll-jupyter) (6.4.12)\n",
      "Requirement already satisfied: pyxll>=5.1.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from pyxll-jupyter) (5.4.4)\n",
      "Requirement already satisfied: PySide6 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from pyxll-jupyter) (6.3.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter>=1.0.0->pyxll-jupyter) (8.0.2)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter>=1.0.0->pyxll-jupyter) (5.3.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter>=1.0.0->pyxll-jupyter) (6.4.4)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter>=1.0.0->pyxll-jupyter) (6.4.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter>=1.0.0->pyxll-jupyter) (6.15.2)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (0.2.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (1.5.5)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (21.3.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (4.10.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (1.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (6.2)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (7.3.5)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (5.1.1)\n",
      "Requirement already satisfied: nbformat in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (5.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (3.0.3)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (23.2.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from notebook>=6.0.0->pyxll-jupyter) (0.14.1)\n",
      "Requirement already satisfied: PySide6-Addons==6.3.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from PySide6->pyxll-jupyter) (6.3.2)\n",
      "Requirement already satisfied: PySide6-Essentials==6.3.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from PySide6->pyxll-jupyter) (6.3.2)\n",
      "Requirement already satisfied: shiboken6==6.3.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from PySide6->pyxll-jupyter) (6.3.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter-client>=5.3.4->notebook>=6.0.0->pyxll-jupyter) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter-client>=5.3.4->notebook>=6.0.0->pyxll-jupyter) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter-core>=4.6.1->notebook>=6.0.0->pyxll-jupyter) (302)\n",
      "Requirement already satisfied: testpath in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.8.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (4.11.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (4.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.5.13)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (2.11.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->pyxll-jupyter) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jinja2->notebook>=6.0.0->pyxll-jupyter) (2.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbformat->notebook>=6.0.0->pyxll-jupyter) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from nbformat->notebook>=6.0.0->pyxll-jupyter) (4.4.0)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from terminado>=0.8.3->notebook>=6.0.0->pyxll-jupyter) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from argon2-cffi->notebook>=6.0.0->pyxll-jupyter) (21.2.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->pyxll-jupyter) (8.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->pyxll-jupyter) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->pyxll-jupyter) (5.9.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->pyxll-jupyter) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.1.6)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->pyxll-jupyter) (4.0.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->pyxll-jupyter) (3.0.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jupyter-console->jupyter>=1.0.0->pyxll-jupyter) (3.0.20)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from qtconsole->jupyter>=1.0.0->pyxll-jupyter) (2.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.18.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (5.1.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.4.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (65.3.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=6.0.0->pyxll-jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=6.0.0->pyxll-jupyter) (21.4.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->pyxll-jupyter) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=5.3.4->notebook>=6.0.0->pyxll-jupyter) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0.0->pyxll-jupyter) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->pyxll-jupyter) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from bleach->nbconvert->jupyter>=1.0.0->pyxll-jupyter) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from packaging->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (3.0.9)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0.0->pyxll-jupyter) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\kannawar shubham\\anaconda3\\envs\\pyspark_env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->pyxll-jupyter) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyxll-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries([x\u001b[38;5;241m.\u001b[39mcount(), x\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum(), x\u001b[38;5;241m.\u001b[39msum(), x\u001b[38;5;241m.\u001b[39mmean(), x\u001b[38;5;241m.\u001b[39mmedian(),  x\u001b[38;5;241m.\u001b[39mstd(), x\u001b[38;5;241m.\u001b[39mvar(), x\u001b[38;5;241m.\u001b[39mmin(), x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.01\u001b[39m), x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.05\u001b[39m),x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.10\u001b[39m),x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m),x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.50\u001b[39m),x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m), x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.90\u001b[39m),x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.95\u001b[39m), x\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.99\u001b[39m),x\u001b[38;5;241m.\u001b[39mmax()], \n\u001b[0;32m      5\u001b[0m                   index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMISS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEAN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDIAN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMIN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP1\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP5\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP10\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP25\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP50\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP75\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP90\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP95\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP99\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m num_summary\u001b[38;5;241m=\u001b[39mtrain_num\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: var_summary(x))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m----> 8\u001b[0m num_summary\u001b[38;5;241m.\u001b[39mto_excel(\u001b[43mwriter\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumeric_variable Summary\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m num_summary\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating Data audit Report\n",
    "# Use a general function that returns multiple values    \n",
    "def var_summary(x):\n",
    "    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n",
    "                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n",
    " \n",
    "num_summary=train_num.apply(lambda x: var_summary(x)).T\n",
    "num_summary.to_excel(writer,'Numeric_variable Summary',index=True)\n",
    "num_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_summary(x):\n",
    "    return pd.Series([x.count(), x.isnull().sum(), x.value_counts()], \n",
    "                  index=['N', 'NMISS', 'ColumnsNames'])\n",
    "\n",
    "cat_summary=train_cat.apply(lambda x: cat_summary(x))\n",
    "cat_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var_test=[key for key in dict(test.dtypes) if dict(test.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "cat_var_test=[key for key in dict(test.dtypes) if dict(test.dtypes)[key] in ['object']]\n",
    "# Train Numerical Data\n",
    "test_num=test[numeric_var_test]\n",
    "\n",
    "# Train Categorical Data\n",
    "test_cat=test[cat_var_test]\n",
    "print numeric_var_test\n",
    "print cat_var_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary=test_num.apply(lambda x: var_summary(x)).T\n",
    "#num_summary.to_excel(writer,'Numeric_variable Summary',index=True)\n",
    "num_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_summary(x):\n",
    "    return pd.Series([x.count(), x.isnull().sum(), x.value_counts()], \n",
    "                  index=['N', 'NMISS', 'ColumnsNames'])\n",
    "\n",
    "cat_summary=test_cat.apply(lambda x: cat_summary(x))\n",
    "cat_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr=pd.DataFrame(train.corr())\n",
    "train_corr.to_excel(writer,'Train_Data Corr',index=True)\n",
    "train_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corr=pd.DataFrame(test.corr())\n",
    "#test_corr.to_excel(writer,'Test_Data Corr',index=True)\n",
    "test_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> -Markdowns are Highly Correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(test.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> -Markdown 4 is  Highly Correlated to Markdown 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Store'].value_counts(normalize=True).plot(kind = 'bar',fig=(4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> -The above graph shows that There are more number of Store 13 and very less number of Store 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Store Size vs Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(kind='line', x='Weekly_Sales', y='Store', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> From this plot, we notice that Store 10 has the highest Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sales vs Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tips = sns.load_dataset('train')\n",
    "sns.barplot(x=train[\"Weekly_Sales\"],y=train[\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> From this plot, we notice that 'type C' stores have fewer sales and 'type A' stores have more sales in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sales vs Deptartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(kind='line', x='Dept', y='Weekly_Sales', alpha=1.5,fig=(4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> From this plot, we notice Deptartment with the highest sales lies between Dept 60 and 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train.isnull().sum()\n",
    "print \"*\"*30\n",
    "print test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Imputing it with its mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['CPI']=test.groupby(['Dept'])['CPI'].transform(lambda x: x.fillna(x.mean()))\n",
    "test['Unemployment']=test.groupby(['Dept'])['Unemployment'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Weekly_Sales=np.where(train.Weekly_Sales>100000, 100000,train.Weekly_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Weekly_Sales.plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "In this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Date Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "test['Date'] = pd.to_datetime(test['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date features\n",
    "train['Date_dayofweek'] =train['Date'].dt.dayofweek\n",
    "train['Date_month'] =train['Date'].dt.month \n",
    "train['Date_year'] =train['Date'].dt.year\n",
    "train['Date_day'] =train['Date'].dt.day \n",
    "\n",
    "test['Date_dayofweek'] =test['Date'].dt.dayofweek\n",
    "test['Date_month'] =test['Date'].dt.month \n",
    "test['Date_year'] =test['Date'].dt.year\n",
    "test['Date_day'] =test['Date'].dt.day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train.Type.value_counts()\n",
    "print \"*\"*30\n",
    "print test.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train.IsHoliday.value_counts()\n",
    "print \"*\"*30\n",
    "print test.IsHoliday.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Converting Categorical Variable 'Type' into Numerical Variable \n",
    "    For A=1 , B=2, C=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\"A\": 1, \"B\": 2, \"C\": 3}\n",
    "for dataset in train_test_data:\n",
    "    dataset['Type'] = dataset['Type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Converting Categorical Variable 'IsHoliday' into Numerical Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {False: 0, True: 1}\n",
    "for dataset in train_test_data:\n",
    "    dataset['IsHoliday'] = dataset['IsHoliday'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Creating Extra Holiday Variable.\n",
    "    If that week comes under extra holiday then 1(=Yes) else 2(=No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making New Holiday Variable Based on Given Data...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Super_Bowl'] = np.where((train['Date']==datetime(2010, 2, 12)) | (train['Date']==datetime(2011, 2, 11)) | (train['Date']==datetime(2012, 2, 10)) | (train['Date']==datetime(2013, 2, 8)),1,0)\n",
    "train['Labour_Day'] = np.where((train['Date']==datetime(2010, 9, 10)) | (train['Date']==datetime(2011, 9, 9)) | (train['Date']==datetime(2012, 9, 7)) | (train['Date']==datetime(2013, 9, 6)),1,0)\n",
    "train['Thanksgiving'] = np.where((train['Date']==datetime(2010, 11, 26)) | (train['Date']==datetime(2011, 11, 25)) | (train['Date']==datetime(2012, 11, 23)) | (train['Date']==datetime(2013, 11, 29)),1,0)\n",
    "train['Christmas'] = np.where((train['Date']==datetime(2010, 12, 31)) | (train['Date']==datetime(2011, 12, 30)) | (train['Date']==datetime(2012, 12, 28)) | (train['Date']==datetime(2013, 12, 27)),1,0)\n",
    "#........................................................................\n",
    "test['Super_Bowl'] = np.where((test['Date']==datetime(2010, 2, 12)) | (test['Date']==datetime(2011, 2, 11)) | (test['Date']==datetime(2012, 2, 10)) | (test['Date']==datetime(2013, 2, 8)),1,0)\n",
    "test['Labour_Day'] = np.where((test['Date']==datetime(2010, 9, 10)) | (test['Date']==datetime(2011, 9, 9)) | (test['Date']==datetime(2012, 9, 7)) | (test['Date']==datetime(2013, 9, 6)),1,0)\n",
    "test['Thanksgiving'] = np.where((test['Date']==datetime(2010, 11, 26)) | (test['Date']==datetime(2011, 11, 25)) | (test['Date']==datetime(2012, 11, 23)) | (test['Date']==datetime(2013, 11, 29)),1,0)\n",
    "test['Christmas'] = np.where((test['Date']==datetime(2010, 12, 31)) | (test['Date']==datetime(2011, 12, 30)) | (test['Date']==datetime(2012, 12, 28)) | (test['Date']==datetime(2013, 12, 27)),1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altering the isHoliday value depending on these new holidays...\n",
    "train['IsHoliday']=train['IsHoliday']|train['Super_Bowl']|train['Labour_Day']|train['Thanksgiving']|train['Christmas']\n",
    "test['IsHoliday']=test['IsHoliday']|test['Super_Bowl']|test['Labour_Day']|test['Thanksgiving']|test['Christmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train.Christmas.value_counts()\n",
    "print train.Super_Bowl.value_counts()\n",
    "print train.Thanksgiving.value_counts()\n",
    "print train.Labour_Day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test.Christmas.value_counts()\n",
    "print test.Super_Bowl.value_counts()\n",
    "print test.Thanksgiving.value_counts()\n",
    "print test.Labour_Day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have Imputed IsHoliday according to Extra holidays..These extra holiday variable has redundant..\n",
    "# Droping the Extra holiday variables because its redundant..\n",
    "dp=['Super_Bowl','Labour_Day','Thanksgiving','Christmas']\n",
    "train.drop(dp,axis=1,inplace=True)\n",
    "test.drop(dp,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Droping irrevelent variable:\n",
    "    -Since we have imputed markdown variables therefore we will not be removing the all markdown variables.\n",
    "    -Removing MarkDown5 because its Highly Skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop=['Unemployment','CPI','MarkDown5']\n",
    "train=train.drop(features_drop, axis=1)\n",
    "test=test.drop(features_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification & Accuracy\n",
    "    Define training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train X= Exery thing except Weekly_Sales\n",
    "train_X=train.drop(['Weekly_Sales','Date'], axis=1)\n",
    "\n",
    "#### train Y= Only Weekly_Sales \n",
    "train_y=train['Weekly_Sales'] \n",
    "test_X=test.drop('Date',axis=1).copy()\n",
    "\n",
    "train_X.shape, train_y.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models & comparing their RMSE values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methood 1..\n",
    "clf = LinearRegression()\n",
    "clf.fit(train_X, train_y)\n",
    "y_pred_linear=clf.predict(test_X)\n",
    "acc_linear=round( clf.score(train_X, train_y) * 100, 2)\n",
    "print ('scorbe:'+str(acc_linear) + ' percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100)\n",
    "clf.fit(train_X, train_y)\n",
    "y_pred_rf=clf.predict(test_X)\n",
    "acc_rf= round(clf.score(train_X, train_y) * 100, 2)\n",
    "print (\"Accuracy: %i %% \\n\"%acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeRegressor()\n",
    "clf.fit(train_X, train_y)\n",
    "y_pred_dt= clf.predict(test_X)\n",
    "acc_dt = round( clf.score(train_X, train_y) * 100, 2)\n",
    "print (str(acc_dt) + ' percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Models\n",
    "Let's compare the accuracy score of all the regression models used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Linear Regression','Random Forest','Decision Tree'],\n",
    "    \n",
    "    'Score': [acc_linear, acc_rf,acc_dt]\n",
    "    })\n",
    "\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting Sales value for test data based on highest score model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction value using Random Forest model..\n",
    "submission = pd.DataFrame({\n",
    "        \"Store_Dept_Date\": test.Store.astype(str)+'_'+test.Dept.astype(str)+'_'+test.Date.astype(str),\n",
    "        \"Weekly_Sales\": y_pred_rf\n",
    "    })\n",
    "\n",
    "submission.to_csv('weekly_sales predicted.csv', index=False)\n",
    "submission.to_excel(writer,'Weekly_sales Pred',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ************* THE END **************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
